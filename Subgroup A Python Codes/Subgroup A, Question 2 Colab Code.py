# -*- coding: utf-8 -*-
"""Subgroup A Question 2 Google Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mb36zHOCfJkdRN-IDPDQPmYhlgf-nKkD

## Importing From Google Drive
"""

# Importing From Google Drive
from google.colab import drive
drive.mount('/content/drive')

"""## Importing The Necessary Packages

For this question, we will need to import several packages, including pandas for data cleaning and transformation, numpy for numefical computation, matplotlib and seaborn for data visualizations, sklearn for Machine Learning Model implementation
"""

# Importing The Necessary Packages

# 1. pandas - to be used for data cleaning
import pandas as pd

# 2. numpy - to be used for numerical computing
import numpy as np

# 3. matplotlib - to be used for data visualizations
import matplotlib.pyplot as plt

# 4. seaborn - to be used for data visualizations
import seaborn as sns

# 5. sklearn - to be used for Machine Learning implementation
import sklearn

"""## Other Settings Implemented Using Pandas

This is an optional step but the following block of code below helps to change the output structure of the code such that

**1) All the columns of the dataset will be printed**

**2) The width of the dataset output is not limited to the display width of Google Colab**

**3) Prevent wrapping the output to multiple lines on Google Colab to improve readibility**
"""

# Show all columns of the dataset when printed
pd.set_option('display.max_columns', None)

# Don't limit the display width of the output
pd.set_option('display.width', None)

# Don't wrap the output to multiple lines
pd.set_option('display.expand_frame_repr', False)

"""## Reading The Excel File From Google Drive To Google Colab"""

# Specify the file path of the excel file
file_path = '/content/drive/MyDrive/uss_survey_responses.xlsx'

# Read the excel file into Google Colab using read_excel
df = pd.read_excel(file_path)

# Display the first few rows of the dataset
print(df.head())

"""## Examining The Number Of Rows And Columns Of The Dataset

We can examine the number of rows and columns of the dataset using `df.shape`, where the first number represents the number of rows and the second number represents the number of columns of the dataset.
"""

# Finding the number of rows and columns of the dataset
num_rows, num_columns = df.shape

# Displaying the results
print("Number of Rows:", num_rows)
print("Number of Columns:", num_columns)

"""We observe that there are 505 rows and 57 columns in the dataset.
The columns include the email address of the survey responders, as well as their responses to the 20 questions in the survey (some of the survey questions have various subparts, hence more than 20 columns altogether). We currently have 505 survey responses in our dataset.

Since the email address is considered highly confidential, in order to maintain data integrity to prevent exposure of information and privacy leaks, we should remove the `email address` column. Also, the `time_entry` column is not really important in our analysis as this column just represents when the respondants have completed the survey (within a period of a few days, all recent entries). We can also remove the column.
"""

# Removing the email addressand time_entry column of the dataset
df = df.drop('Email Address', axis = 1)
df = df.drop('time_entry', axis = 1)

# Displaying the first few rows of the updated dataset
print(df.head())

"""## What Each Column Of The Dataset Represent (From The Survey Questions)

Here is a description of the what each of the various columns of the dataset represent:

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        table {
            width: 50%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid black;
            padding: 10px;
            text-align: center;
        }
    </style>
</head>
<body>
    <table>
        <thead>
            <tr>
                <th>Column Name</th>
                <th>Description Of Column</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>q1</td>
                <td>Which type of theme park visitor best describes you?</td>
            </tr>
            <tr>
                <td>q2_1</td>
                <td>What is your age range?</td>
            </tr>
            <tr>
                <td>q2_2</td>
                <td>What is your gender?</td>
            </tr>
            <tr>
                <td>q3</td>
                <td>Are you a tourist or a local?</td>
            </tr>
            <tr>
                <td>q4_1</td>
                <td>For the category on thrill rides, what is the average time you queued for?</td>
            </tr>
            <tr>
                <td>q4_2</td>
                <td>For the category on interactive exhibits, what is the average time you queued for?</td>
            </tr>
            <tr>
                <td>q4_3</td>
                <td>For the category on performances, what is the average time you queued for?</td>
            </tr>
            <tr>
                <td>q4_4</td>
                <td>For the category on food and dining, what is the average time you queued for?</td>
            </tr>
            <tr>
                <td>q5_1</td>
                <td>For the category on thrill rides, what is considered an unacceptable wait time? (Integer in Minutes)</td>
            </tr>
            <tr>
                <td>q5_2</td>
                <td>For the category on interactive exhibits, what is considered an unacceptable wait time? (Integer in Minutes)</td>
            </tr>
            <tr>
                <td>q5_3</td>
                <td>For the category on performances, what is considered an unacceptable wait time? (Integer in Minutes)</td>
            </tr>
            <tr>
                <td>q5_4</td>
                <td>For the category on food and dining, what is considered an unacceptable wait time? (Integer in Minutes)</td>
            </tr>
            <tr>
                <td>q6</td>
                <td>The type of attractions you enjoy the most when visiting USS</td>
            </tr>
            <tr>
                <td>q7</td>
                <td>Factors that will influence your decision to visit a theme park like USS?</td>
            </tr>
            <tr>
                <td>q8</td>
                <td>What type of events influence your decision to visit?</td>
            </tr>
            <tr>
                <td>q9</td>
                <td>How long do you usually spend at USS? (Integer in Hours)</td>
            </tr>
            <tr>
                <td>q10</td>
                <td>When do you usually visit theme parks or attractions like USS?</td>
            </tr>
            <tr>
                <td>q11</td>
                <td>When do you typically purchase meals or snacks at the eateries/restaurants?</td>
            </tr>
            <tr>
                <td>q12</td>
                <td>How do you usually navigate a theme park like USS?</td>
            </tr>
            <tr>
                <td>q13</td>
                <td>Would you be willing to wear a digital watch given by USS to track your location and activity?</td>
            </tr>
            <tr>
                <td>q14_1</td>
                <td>At what time of the day do you usually visit roller coasters?</td>
            </tr>
            <tr>
                <td>q14_2</td>
                <td>At what time of the day do you usually visit water rides?</td>
            </tr>
            <tr>
                <td>q14_3</td>
                <td>At what time of the day do you usually visit 3D/4D experiences?</td>
            </tr>
            <tr>
                <td>q14_4</td>
                <td>At what time of the day do you usually visit performances?</td>
            </tr>
            <tr>
                <td>q14_5</td>
                <td>At what time of the day do you usually visit roadshows?</td>
            </tr>
            <tr>
                <td>q14_6</td>
                <td>At what time of the day do you usually visit eateries and restaurants?</td>
            </tr>
            <tr>
                <td>q14_7</td>
                <td>At what time of the day do you usually visit souvenir shops?</td>
            </tr>
            <tr>
                <td>q14_8</td>
                <td>At what time of the day do you usually visit other rides (carousel rides, teacup rides etc.)?</td>
            </tr>
            <tr>
                <td>q15</td>
                <td>How likely are you to recommend USS to others?</td>
            </tr>
            <tr>
                <td>q16_1</td>
                <td>How satisfied are you with the overall service of the queuing system?</td>
            </tr>
            <tr>
                <td>q16_2</td>
                <td>How satisfied are you with the overall service of retail experience?</td>
            </tr>
            <tr>
                <td>q16_3</td>
                <td>How satisfied are you with the overall service of eateries/restaurants?</td>
            </tr>
            <tr>
                <td>q16_4</td>
                <td>How satisfied are you with the overall service of photo taking exhibitions?</td>
            </tr>
            <tr>
                <td>q16_5</td>
                <td>How satisfied are you with the overall service of entertainment attractions?</td>
            </tr>
            <tr>
                <td>q17_1</td>
                <td>Give an overall rating for ticketing information accessibility</td>
            </tr>
            <tr>
                <td>q17_2</td>
                <td>Give an overall rating for rides and attractions</td>
            </tr>
            <tr>
                <td>q17_3</td>
                <td>Give an overall rating for entertainment and performances</td>
            </tr>
            <tr>
                <td>q17_4</td>
                <td>Give an overall rating for food and beverage</td>
            </tr>
            <tr>
                <td>q17_5</td>
                <td>Give an overall rating for merchandise and shopping</td>
            </tr>
            <tr>
                <td>q17_6</td>
                <td>Provide an overall rating for crowd management, comfort and staff helpfulness</td>
            </tr>
            <tr>
                <td>q18_1</td>
                <td>For ticketing information accessibility, which of the following services are you not satisfied with?</td>
            </tr>
            <tr>
                <td>q18_2</td>
                <td>For rides and attractions, which of the following services are you not satisfied with?</td>
            </tr>
            <tr>
                <td>q18_3</td>
                <td>For entertainment and performances, which of the following services are you not satisfied with?</td>
            </tr>
            <tr>
                <td>q18_4</td>
                <td>For food and beverage, which of the following services are you not satisfied with?</td>
            </tr>
            <tr>
                <td>q18_5</td>
                <td>For merchandise and shopping, which of the following services are you not satisfied with?</td>
            </tr>
            <tr>
                <td>q18_6</td>
                <td>For crowd management, comfort and staff helpfulness, which of the following services are you not satisfied with?</td>
            </tr>
            <tr>
                <td>q19_1</td>
                <td>How important is ticketing information accessibility to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q19_2</td>
                <td>How important is crowd management to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q19_3</td>
                <td>How important is staff helpfulness to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q19_4</td>
                <td>How important is safety and cleanliness to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q19_5</td>
                <td>How important is rides and attractions to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q19_6</td>
                <td>How important is food and beverage to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q19_7</td>
                <td>How important is merchandise and shopping to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q19_8</td>
                <td>How important is entertainment and performances to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q20</td>
                <td>Is there any other feedback about your USS experience that you want to mention?</td>
            </tr>
        </tbody>
    </table>
</body>
</html>

****************************************************************************************************************************************************************

### Business Question 2: Creating a Guest Segmentation Model

#### Overview Of Guest Segmentation Model

A Guest Segmentation Model is a data-driven approach that can be used by theme parks, such as USS, to classify visitors into different groups based on shared characteristics, behaviors, and preferences. This model leverages data collected from various sources, including ticket purchases, ride preferences, spending patterns and demographic details.

By applying machine learning and clustering techniques such as K-Means, hierarchical clustering, or decision trees, the model can identify distinct guest segments, allowing USS to tailor its services and experiences to meet the unique needs of each group.

Typically, guests can be segmented into various categories (known as clusters)based on their characteristics. Each segment might have different expectations, for instance families may prioritize child-friendly attractions and short wait times, while thrill-seekers focus on high-intensity rides. By understanding these distinctions, USS can enhance guest experiences, personalize marketing campaigns, and optimize operational efficiency.

Now that we have a good overview of the key factors, let's investigate the data at a deeper level, to get a better idea of our guests.

To tackle the business question, we need to
  
Step 1: **Feature engineering, Data Cleaning and Data Preprocessing** to retain the most insightful rows and scale data, in preparation for later steps.

Step 2: **Principle Compoment Analysis(PCA)** for dimensionality reduction.

Step 3: **Parameter tuning** for model, using elbow method and other visualisation methods to identify the ideal number of clusters.

Step 4: **K-means clustering** model implementation to label the different clusters.

Step 5: **Analysis of various guest segments** and their key distinctions and characteristics.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### Step 1: Feature engineering, Data Cleaning and Data Preprocessing

##### Selecting important columns for Machine Learning Model Implementation

Examining the dataset columns, there are several columns that are important while other columns are deemed unnecesary and might add variance to the model that we will implement subsequently, leading to a higher possibility of overfitting.

**Important Columns To Be Used For Machine Learning Model:**

Columns Pertaining To **Visitor Demographics**:
*   `q1`: Which type of theme park visitor best describes you?
*   `q2_1`: What is your age range?
*   `q3`: Are you a tourist or a local?

Columns Pertaining To **Unacceptable Waiting Times**:
*   `q5_1`: For the category on thrill rides, what is considered an unacceptable wait time?
*   `q5_2`: For the category on interactive exhibits, what is considered an unacceptable wait time?
*   `q5_3`: For the category on performances, what is considered an unacceptable wait time?
*   `q5_4`: For the category on food and dining, what is considered an unacceptable wait time?

Columns Pertaining To **Important Factors Affecting Visitation**:
*   `q6`: The type of attractions you enjoy the most when visiting USS
*   `q7`: Factors that will influence your decision to visit a theme park like USS?

Columns Pertaining To **Navigation And Duration Spent At USS**:
*   `q9`: How long do you usually spend at USS?
*   `q10`: When do you usually visit theme parks or attractions like USS?
*   `q12`: How do you usually navigate a theme park like USS?

Columns Pertaining To **Visitor Recommendation**:
*   `q15`: How likely are you to recommend USS to others?

Columns Pertaining To **Service Experience In USS**:
*   `q16_1`: How satisfied are you with the overall service of the queuing system?
*   `q16_2`: How satisfied are you with the overall service of retail experience?
*   `q16_3`: How satisfied are you with the overall service of eateries/restaurants?
*   `q16_4`: How satisfied are you with the overall service of photo taking
exhibitions?
*   `q16_5`: How satisfied are you with the overall service of entertainment attractions?

Columns Pertaining To **Importance Of Services**:
*   `q19_1`: How important is ticketing information accessibility to your overall satisfaction?
*   `q19_2`: How important is crowd management to your overall satisfaction?
*   `q19_3`: How important is staff helpfulness to your overall satisfaction?
*   `q19_4`: How important is safety and cleanliness to your overall satisfaction?
*   `q19_5`: How important is rides and attractions to your overall satisfaction?
*   `q19_6`: How important is food and beverage to your overall satisfaction?
*   `q19_7`: How important is merchandise and shopping to your overall satisfaction?
*   `q19_8`: How important is entertainment and performances to your overall satisfaction?

Why are these columns important for a Guest Segmentation Model?

1. **Visitor Demographics:** Demographic data such as age and nationality provides essential insights into visitor preferences and behaviors. For example, families with young children may prefer kid-friendly attractions, while international tourists might prioritize high-profile rides and express passes. These insights enable the model to group guests into meaningful segments for targeted marketing and service enhancements.

2. **Unacceptable Waiting Times:** Long wait times can significantly impact guest satisfaction and overall experience. By analyzing how different visitor segments perceive acceptable vs. unacceptable wait times, the model can help USS optimize queue management strategies, such as implementing virtual queues, adjusting ride schedules, or promoting lesser-known attractions to redistribute crowds more effectively.

3. **Important Factors Affecting Visitation:** Factors like ticket pricing, weather conditions, event schedules, and seasonal demand influence a guest's decision to visit USS. Identifying these patterns allows the model to predict peak visit times and suggest personalized promotions, ensuring higher visitor turnout and better park resource allocation.

4. **Navigation Patterns:** Tracking how guests move through the park helps identify popular routes, congestion points, and underutilized areas. If certain segments (families with young children) tend to stick to specific zones, USS can improve signage, create more efficient pathways, and strategically place amenities to enhance convenience.

5. **Duration Spent at USS:** Understanding how long different visitor segments stay in the park allows USS to optimize attraction engagement and service offerings. For example, tourists who stay the whole day might benefit from meal package deals, while short-stay visitors could be targeted with express pass promotions. This data also helps predict visitor fatigue levels, leading to better crowd control and scheduling of entertainment options to keep guests engaged.

4. **Visitor Recommendation:** Guests often rely on reviews and recommendations when planning their visits. Tracking whether visitors recommend USS to others  helps the model assess satisfaction levels and identify pain points for specific segments. This data is crucial for enhancing service offerings and maintaining brand reputation.

5. **Service Experience in USS:** Understanding visitor feedback on staff friendliness, cleanliness, ride operations, and overall service quality helps USS improve guest engagement strategies. If certain segments report dissatisfaction with food service or park navigation, targeted solutions can be implemented to address specific concerns, leading to a more enjoyable experience.

6. **Importance of Services:** Different guest segments value services differently. Some may prioritize accessibility features or dining variety, while others focus on photo opportunities and exclusive merchandise. By identifying which services matter most to each segment, USS can allocate resources efficiently, ensuring that high-value services are improved and promoted to the right audience.
"""

# Extracting the important columns for Machine Learning Model
data = df[['q1', 'q2_1', 'q3',                             # Demographics
           'q5_1', 'q5_2', 'q5_3', 'q5_4',                 # Unacceptable wait time
           'q6', 'q7',                                     # Important factors
           'q9', 'q10', 'q12',                             # Navigation And Duration Spent
           'q15',                                          # Visitor Recommendation
           'q16_1', 'q16_2', 'q16_3', 'q16_4', 'q16_5',    # Service Experience
           'q19_1', 'q19_2', 'q19_3', 'q19_4',
           'q19_5', 'q19_6', 'q19_7', 'q19_8'              # Importance Of Services
           ]]

print(data.head())

"""--------------------------------------------------------------------------------

**Columns That Will Not Be Used In The Machine Learning Model:**

Columns Pertaining To **Average Waiting Times**:
*   `q4_1`: For the category on thrill rides, what is the average time you queued for?
*   `q4_2`: For the category on interactive exhibits, what is the average time you queued for?
*   `q4_3`:	For the category on performances, what is the average time you queued for?
*   `q4_4`:	For the category on food and dining, what is the average time you queued for?

Columns Pertaining To **Types Of Events**:
*   `q8`: What type of events influence your decision to visit USS?

Columns Pertaining To **Time Of Eating/Dining**:
*   `q11`: When do you typically purchase meals or snacks at the eateries/restaurants?

Columns Pertaining To **User's Preference For Digital Watch Tracking**:
*   `q13`: Would you be willing to wear a digital watch given by USS to track your location and activity?

Columns Pertaining To **Time Of Day To Visit Attractions At USS**:
*   `q14_1`: At what time of the day do you usually visit roller coasters?
*   `q14_2`: At what time of the day do you usually visit water rides?
*   `q14_3`: At what time of the day do you usually visit 3D/4D experiences?
*   `q14_4`: At what time of the day do you usually visit performances?
*   `q14_5`: At what time of the day do you usually visit roadshows?
*   `q14_6`: At what time of the day do you usually visit eateries and restaurants?
*   `q14_7`: At what time of the day do you usually visit souvenir shops?
*   `q14_8`: At what time of the day do you usually visit other rides?

Columns Pertaining To **Overall Rating By Visitors**:
*   `q17_1`: Give an overall rating for ticketing information accessibility
*   `q17_2`: Give an overall rating for rides and attractions
*   `q17_3`: Give an overall rating for entertainment and performances
*   `q17_4`: Give an overall rating for food and beverage
*   `q17_5`: Give an overall rating for merchandise and shopping
*   `q17_6`: Provide an overall rating for crowd management, comfort and staff helpfulness

Columns Pertaining To **Unsastisfaction Reasons From Customers**:
*   `q18_1`: For ticketing information accessibility, which of the following services are you not satisfied with?
*   `q18_2`: For rides and attractions, which of the following services are you not satisfied with?
*   `q18_3`: For entertainment and performances, which of the following services are you not satisfied with?
*   `q18_4`: For food and beverage, which of the following services are you not satisfied with?
*   `q18_5`: For merchandise and shopping, which of the following services are you not satisfied with?
*   `q18_6`: For crowd management, comfort and staff helpfulness, which of the following services are you not satisfied with?

Why are these columns not included in the Guest Segmentation Model?

1. **Average Waiting Times:** While waiting times impact guest experience, they are dynamic and operational factors rather than visitor-specific attributes. Guest segmentation focuses on identifying visitor groups, whereas waiting time analysis is better suited for real-time optimization models or queue management systems.

2. **Types of Events:** Event preferences are situational and can change based on seasonal promotions, new attractions, or holiday schedules. Since segmentation models aim to classify long-term guest behaviors, this column is better suited for event planning and marketing analytics rather than core segmentation.

3. **Time of Eating/Dining:** While dining habits can be useful for food service optimization, they do not directly define visitor segments. The same guest could eat at different times depending on their itinerary, making this data too variable for effective segmentation. Instead, food sales and crowd trends can be used for restaurant scheduling.

4. **User's Preference for Digital Watch Tracking:** Whether a visitor prefers a digital watch, smartphone, or park-provided app for tracking does not significantly influence behavioral segmentation. This is more relevant for technology adoption analysis rather than defining distinct guest groups.

5. **Time of Day to Visit Attractions at USS:** Attraction visit timing is context-dependent and influenced by crowd flow, weather, and ride availability. While it helps with park operations, it does not fundamentally define a visitor persona. Segmentation focuses on who the guests are, rather than when they experience specific rides.

6. **Overall Rating by Visitors:** Satisfaction ratings reflect individual experiences rather than segment characteristics. While useful for tracking park performance, this data is too subjective and post-visit focused to be a reliable input for segmentation models. Instead, it is better suited for customer feedback analysis.

7. **Unsatisfaction Reasons from Customers:** While valuable for service improvement, dissatisfaction reasons are reactionary data points rather than proactive indicators of visitor segmentation. This information is better used in customer service improvements and complaint resolution strategies rather than clustering guest types.

--------------------------------------------------------------------------------

##### Data Cleaning And Preprocessing

Now that we have selected the features we intend to use for our model, we will now proceed to clean and preprocess the data. There are several steps that we will use to clean and preprocess the data.

Data Cleaning:
*   Handling And Removing `NA` Values
*   Handling And Removing **Outliers**

Data Preprocessing:
*   Scaling Numerical Features
*   Encoding Categorical Variables

--------------------------------------------------------------------------------

###### Handling `NA` values

We can first check if there are any `NA` values for each column of the dataset. This can be done using the `.isna().sum()` method on the dataframe `data`.
"""

# Count the number of NA values in each column
na_counts = data.isna().sum()

# Display the results
print("Number Of NA Values:")
print()
print(na_counts)

"""There are indeed several NA values for some columns of the `data` dataframe. We observe that there is 1 `NA` value for `q16_2`, 1 `NA` value for `q16_4`, 1 `NA` value for `q16_5` and 4 `NA` values for `q16_3`.

We can either remove the rows containing the `NA` values of perform some imputation using mean, median or mode for the `NA` values. Since there are very few rows that contain `NA` values with respect to the entire dataset containing more than 500 rows, it is better to remove these rows with `NA` values.
"""

# Drop all rows with NA values
data_clean = data.dropna()

# Printing the number of rows of the resulting dataset without NA values
print(len(data_clean))

"""There are now 501 rows of data left, instead of the initial 505 before the rows containing the `NA` values are removed. This shows that 4 rows of data contain one or more `NA` values in their columns. Now the dataframe is free from `NA` values.

--------------------------------------------------------------------------------

###### Handling Outliers For Numerical Columns

To detect any possible outliers in the dataset, we need to check if there are any anomalies (example: extremely large and small values, negative value in time taken etc.). The values are either irrelevant or ridiculously large/small that might affect the results of the Machine Learning model.

Before checking for outliers, we need to first identify the numerical columns of the dataset. Some columns of the dataset are categorical and they exist within a fixed range/category of answers, hence no checking is needed.

The numerical columns that have no fixed range are:

*   `q5_1`: For the category on thrill rides, what is considered an unacceptable wait time?
*   `q5_2`: For the category on interactive exhibits, what is considered an unacceptable wait time?
*   `q5_3`: For the category on performances, what is considered an unacceptable wait time?
*   `q5_4`: For the category on food and dining, what is considered an unacceptable wait time?
*   `q9`: How long do you usually spend at USS?
"""

# Identifying The Numerical Columns Of The Dataframe
numerical = ['q5_1', 'q5_2', 'q5_3', 'q5_4', 'q9']

# Observing The Summary Statistics Of These Columns
print(data_clean[numerical].describe())

"""We observe that for the values on unacceptable waiting times (`q5_1`, `q5_2`, `q5_3`, `q5_4`), the maximum values have the values 120118, 94226, 85884 and 125439 respectively, which are way too large. Even with exteremely high demands, the maximum waiting time for attractions is ususally under 4 hours. The minumum values are 5 for `q5_1` and 0 for `q5_2`, `q5_3` and `q5_4`, which are considered acceptable values as some visitors might not want to wait/do not have the patience to be in queues.

Similarly for `q9`, the maximum value is 24000 hours, which again does not make sense in the context of length of visitation of USS. The maximum number of hours that USS is open is around 14 hours (from 8am in the morning to 10pm at night). Hence, any value that is above 14 is considered irrelevant. The minimum value is 0, which also does not make sense as 0 hours represent not visiting USS at all. The minimum number of hours is 1.

Due to the exteremely high values for some entries, the standard deviation also has large values.

Acceptable Range Of Values:
*   `q5_1`: 0 to 300 (5 hours maximum)
*   `q5_2`: 0 to 300 (5 hours maximum)
*   `q5_3`: 0 to 500 (5 hours maximum)
*   `q5_4`: 0 to 500 (5 hours maximum)
*   `q9`: 1 to 14 (1 hour minimum, 14 hours maximum)

Any rows that do not fit these conditions and value ranges will also be removed.


"""

# Convert relevant columns to numeric, forcing errors='coerce' to handle invalid values
cols_to_check = ['q5_1', 'q5_2', 'q5_3', 'q5_4', 'q9']
data_clean[cols_to_check] = data_clean[cols_to_check].apply(pd.to_numeric, errors='coerce')

# Drop any rows where conversion created NaN values (invalid values)
data_clean = data_clean.dropna(subset=cols_to_check)

# Apply filtering conditions
data_clean = data_clean[
    (data_clean['q5_1'].between(0, 300)) &
    (data_clean['q5_2'].between(0, 300)) &
    (data_clean['q5_3'].between(0, 500)) &
    (data_clean['q5_4'].between(0, 500)) &
    (data_clean['q9'].between(1, 14))
]

# Printing the number of rows of the resulting dataset without NA and outliers
print(len(data_clean))

"""There are now 499 rows of data left, instead of the initial 501 before the rows containing outlier values are removed. This shows that 2 rows of data contain one or more outliers in their columns that do not fit the corresponding ranges. Now the dataframe is free from `NA` values, as well as outliers for the numerical variables.

--------------------------------------------------------------------------------

###### Handling outliers For Categorical Columns

The categorical columns of the dataframe include:

*   `q1`: Which type of theme park visitor best describes you?
*   `q2_1`: What is your age range?
*   `q3`: Are you a tourist or a local?
*   `q6`: The type of attractions you enjoy the most when visiting USS
*   `q7`: Factors that will influence your decision to visit a theme park like USS?
*   `q10`: When do you usually visit theme parks or attractions like USS?
*   `q12`: How do you usually navigate a theme park like USS?

However, only three columns, `q7`, `q10` and `q12`, do not have fixed categories as the survey respondants are able to type in their own responses. For the remaining questions, there are fixed choices which the users can select, but they are not able to input extra responses. This means that we are sure that the columns `q1`, `q2_1`, `q3` and `q7` do not have any anomalies.

We need to check for any anomalies for `q7`, `q10` and `q12`.

Before identifying the anomalies, we need to handle the complex data for the columns `q6` and `q10`. This is because some options for `q6` and `q10` involve the use of `brackets ()` that will cause the data to be handled incorrectly if not cleaned accurately. Also, some options for `q6` and `q10` involve capital letters and to standardize, we will change all the values and words to lowercase.

The code below performs data cleaning and text standardization on the columns `q6` and `q10` in the `data_clean` dataset. First, it removes any text enclosed in parentheses (e.g., "Teacup Ride (Suspended Coasters)") using **regex** for both columns, leaving just the core ride or event name. Afterward, it converts all string values in the entire DataFrame to lowercase to ensure uniformity in text formatting, facilitating easier comparison and analysis. This ensures that all text data is cleaned and standardized before further analysis.
"""

# Disable chained assignment warning
pd.options.mode.chained_assignment = None

# Remove text in parentheses (e.g., 'Teacup Ride' to 'Teacup Ride')
# for column 'q6' and keep only the main ride name
data_clean['q6'] = data_clean['q6'].str.replace(r'\(.*\)', '', regex=True)

# Remove text in parentheses for column 'q10' (e.g., 'Halloween' to 'Halloween')
data_clean['q10'] = data_clean['q10'].str.replace(r'\(.*\)', '', regex=True)

# Convert all values in the dataset to lowercase to standardize the text
data_clean = data_clean.apply(lambda x: x.astype(str).str.lower())

# Display cleaned dataset
print(data_clean.head())

"""Now, we can proced to check the possible anomalies for `q7`, `q10` and `q12` of the dataset.

We first processes each column by splitting the comma-separated string values into lists, and then stripping any extra spaces around the entries. Next, we converts these lists into sets of unique values (`all_7`, `all_10`, `all_12`). The sets are compared to predefined sets of valid values for each respective column and finds the differences (outliers) using the set subtraction operation. Finally, it prints the outliers for each column, helping to identify any values that do not belong to the predefined categories for better data quality and consistency.
"""

# For Question 7
# Extract unique values from column 'q7' by splitting comma-separated strings and stripping leading/trailing spaces
all_7 = set([x for sub in data_clean['q7'].apply(lambda x: [j.strip() for j in x.split(",")]) for x in sub])

# Identify values in 'q7' that are not in the predefined valid categories (outliers)
out_7 = all_7 - set(['weather conditions', 'safety and cleanliness', 'attraction variety', 'cost and ticket prices',
                     'special events', 'reputation and reviews', 'holiday seasons', 'location and accessibility', 'wait times for rides'])

# Print the outliers for 'q7'
print(f'Outliers for q7: {out_7}')

# For Question 10
# Extract unique values from column 'q10' by splitting comma-separated strings and stripping spaces
all_10 = set([x for sub in data_clean['q10'].apply(lambda x: [j.strip() for j in x.split(",")]) for x in sub])

# Identify values in 'q10' that are not in the predefined valid categories (outliers)
out_10 = all_10 - set(['weather conditions', 'special events', 'public holidays', 'school holidays', 'weekends', 'weekdays'])

# Print the outliers for 'q10'
print(f'Outliers for q10: {out_10}')

# For Question 12
# Extract unique values from column 'q12' by splitting comma-separated strings and stripping spaces
all_12 = set([x for sub in data_clean['q12'].apply(lambda x: [j.strip() for j in x.split(",")]) for x in sub])

# Identify values in 'q12' that are not in the predefined valid categories (outliers)
out_12 = all_12 - set(['spontaneous exploration', 'following shortest queue', 'pre-planned route'])

# Print the outliers for 'q12'
print(f'Outliers for q12: {out_12}')

"""Observing for outliers, we find that:

*   **2 anomalies** for `q7`: "aesthetics" and "thrill factor (not to be confused with scare factor)"
*   **0 anomalies** for `q10` -> Gives an empty set
*   **1 anomaly** for `q12`: "mix of pre-planning and spontaneous exploration"

Similarly, these number of outliers (3 outliers) make up a small proportion of the dataset (maximum of 3 rows out of 499 rows which is less than 1%). It is best to remove these rows containing the outliers to prevent inaccuracies in the Machine Learning Model subsequently.

We filter out rows from the data_clean dataset that contain specific anomalous values in the columns `q7`, `q10` and `q12`. We first define lists of anomalies for each column, then we apply a lambda function to each entry in the respective columns to check if any of the anomaly values are present. The `apply()` function iterates through the entries, and the `any()` function checks if any anomaly in the list exists within the string. Using the negation operator (~), it excludes rows containing the identified anomalies by creating a boolean mask, and the filtered dataset is stored in `data_clean`, which contains only the rows that do not have the specified anomalies.
"""

# List of anomalies for each column
q7_anomalies = ['aesthetics', 'thrill factor (not to be confused with scare factor)']
q10_anomalies = []
q12_anomalies = ['mix of pre-planning and spontaneous exploration']

# Filter the dataset by excluding rows containing these anomalies
data_clean = data_clean[
    ~data_clean['q7'].apply(lambda x: any(anomaly in x for anomaly in q7_anomalies)) &
    ~data_clean['q10'].apply(lambda x: any(anomaly in x for anomaly in q10_anomalies)) &
    ~data_clean['q12'].apply(lambda x: any(anomaly in x for anomaly in q12_anomalies))
]

# Display the number of rows of the filtered dataset
print(len(data_clean))

"""We observe that after filtering out the anomaly values for the categorical variables, the number of rows of the dataset reduced by 3 from 499 rows to 496 rows, indicating that the three rows containing the outliers have been successfully removed. Our datset is now free of `NA`, as well as outliers for both numerical and categorical variables.

--------------------------------------------------------------------------------

###### Encoding Categorical Variables

Now our data is cleaned and we can proceed to the next step - data preprocessing. For data processing, we will perform two operations, encoding categorical variables, as well as scaling numerical variables. We will first encode the categorical variables.

The categorical variables are:

*   `q1`: Which type of theme park visitor best describes you?
*   `q2_1`: What is your age range?
*   `q3`: Are you a tourist or a local?
*   `q6`: The type of attractions you enjoy the most when visiting USS
*   `q7`: Factors that will influence your decision to visit a theme park
*   `q10`: When do you usually visit theme parks or attractions like USS?
*   `q12`: How do you usually navigate a theme park like USS?

We will encode all the categorical variables using **One-Hot Encoding**:

**One-Hot Encoding** is a technique used to convert categorical variables into a format that can be used by machine learning algorithms, which typically require numerical input. In One-Hot Encoding, each unique category in a categorical variable is represented as a binary vector. For each category, a new binary column is created, and for each row, a "1" is placed in the column corresponding to the category the row belongs to, while all other columns for that category are filled with "0". This process ensures that the categorical data is represented numerically while preserving the distinctness of each category, making it suitable for algorithms that rely on numerical input, like most machine learning models.

The code below performs one-hot encoding on multiple categorical columns in the `data_clean` dataset, specifically the columns listed in the categorical list. First, it uses `MultiLabelBinarizer` to encode the `q1` column by splitting comma-separated values and adding a prefix (`q1_`) to each category, resulting in binary columns for each unique category. The process is repeated for each subsequent categorical column in the list (`q2_1`, `q3`, `q6`, `q7`, `q10`, `q12`), where each column's categories are also split by commas, prefixed with the column name, and transformed into binary columns. The resulting one-hot encoded columns are concatenated into a new DataFrame (`data_encode_cat`). The final DataFrame contains all the one-hot encoded variables from the specified categorical columns.
"""

# List of categorical columns to encode
categorical = ['q1', 'q2_1', 'q3', 'q6', 'q7', 'q10', 'q12']

# Initialize MultiLabelBinarizer for the first column 'q1'
mlb = sklearn.preprocessing.MultiLabelBinarizer()

# Apply one-hot encoding to 'q1' by splitting comma-separated values and adding column prefixes, then store in a DataFrame
transformed_data = mlb.fit_transform(data_clean['q1'].apply(lambda x: ['q1_'+sec.strip() for sec in x.split(",")]))
data_encode_cat = pd.DataFrame(transformed_data, columns = mlb.classes_, index = data_clean.index)

# Initialize a dictionary to store MultiLabelBinarizer for each column in 'categorical' (excluding 'q1')
mlb = {}

# Iterate over the rest of the categorical columns (from 'q2_1' to 'q12')
for c in categorical[1:]:
    # Initialize the MultiLabelBinarizer for each column
    mlb[c] = sklearn.preprocessing.MultiLabelBinarizer()

    # Apply one-hot encoding to each categorical column, splitting values by commas, and add column prefixes
    temp = pd.DataFrame(mlb[c].fit_transform(data_clean[c].apply(lambda x: [c+'_'+sec.strip() for sec in x.split(",")])),
                       columns=mlb[c].classes_,
                       index=data_clean[c].index)

    # Concatenate the one-hot encoded columns to the result DataFrame
    data_encode_cat = pd.concat([data_encode_cat, temp], axis=1)

# Display the final one-hot encoded DataFrame
print(data_encode_cat.head())

"""--------------------------------------------------------------------------------

###### Scaling Numerical Variables

The numerical variables of the dataset are:

*   `q5_1`: For the category on thrill rides, what is considered an unacceptable wait time?
*   `q5_2`: For the category on interactive exhibits, what is considered an unacceptable wait time?
*   `q5_3`: For the category on performances, what is considered an unacceptable wait time?
*   `q5_4`: For the category on food and dining, what is considered an unacceptable wait time?
*   `q9`: How long do you usually spend at USS?
*   `q15`: How likely are you to recommend USS to others?
*   `q16_1`: How satisfied are you with the overall service of the queuing system?
*   `q16_2`: How satisfied are you with the overall service of retail experience?
*   `q16_3`: How satisfied are you with the overall service of eateries/restaurants?
*   `q16_4`: How satisfied are you with the overall service of photo taking
exhibitions?
*   `q16_5`: How satisfied are you with the overall service of entertainment attractions?
*   `q19_1`: How important is ticketing information accessibility to your overall satisfaction?
*   `q19_2`: How important is crowd management to your overall satisfaction?
*   `q19_3`: How important is staff helpfulness to your overall satisfaction?
*   `q19_4`: How important is safety and cleanliness to your overall satisfaction?
*   `q19_5`: How important is rides and attractions to your overall satisfaction?
*   `q19_6`: How important is food and beverage to your overall satisfaction?
*   `q19_7`: How important is merchandise and shopping to your overall satisfaction?
*   `q19_8`: How important is entertainment and performances to your overall satisfaction?

We will scaling the numerical columns using Min-Max Scaler.

**Min-max scaling**, also known as normalization, is a technique used to rescale the features of a dataset into a specified range, typically between 0 and 1. This is achieved by subtracting the minimum value of each feature and dividing by the range (the difference between the maximum and minimum values) for that feature. This process ensures that all features are on the same scale, preventing certain features from dominating others. Min-max scaling is particularly useful when features have different units or magnitudes, and it helps improve the convergence speed of some algorithms.

The code below scales the numerical columns of the `data_clean` dataset using min-max scaling, which normalizes each feature to a range between 0 and 1. It first creates a list of columns to be scaled (numerical), and then selects these columns from the dataset into `data_scale_num`. A `MinMaxScaler` from the `sklearn.preprocessing` module is initialized to transform the selected data. The `fit_transform()` method is applied to scale the data, ensuring that each value in the columns is transformed based on the minimum and maximum values of the column. The scaled data is then assigned back to `data_scale_num`, and the result is printed to show the normalized numerical features.
"""

# Define a list of numerical columns to scale
numerical = ['q5_1', 'q5_2', 'q5_3', 'q5_4', 'q9', 'q15', 'q16_1', 'q16_2', 'q16_3', 'q16_4', 'q16_5',
             'q19_1', 'q19_2', 'q19_3', 'q19_4', 'q19_5', 'q19_6', 'q19_7', 'q19_8']

# Select only the numerical columns from the dataset
data_scale_num = data_clean[numerical]

# Initialize a MinMaxScaler object
scaler = sklearn.preprocessing.MinMaxScaler()

# Apply min-max scaling to the selected numerical columns
data_scale_num[:] = scaler.fit_transform(data_scale_num)

# Print the scaled numerical data
print(data_scale_num.head())

"""--------------------------------------------------------------------------------

###### Merging The Categorical And Numerical Columns

After encoding the categorical variables, as well as scaling the numerical variables, we can then merging the two dataframes together using the `.concat()` method.
"""

# Concatenate the encoded categorical data (data_encode_cat) and the scaled numerical data (data_scale_num)
# The axis=1 argument ensures that the concatenation is done column-wise (horizontally)
data_c = pd.concat([data_encode_cat, data_scale_num], axis=1)

# Display the concatenated dataset (data_c)
print(data_c.head())

"""--------------------------------------------------------------------------------

#### Step 2: Performing Principal Component Analysis (PCA)

The Machine Learning Model that we will be implementing is **K-Means Clustering**.

**K-Means clustering** is an unsupervised machine learning algorithm used to partition a dataset into a predefined number of clusters, where each cluster contains data points that are similar to one another. The algorithm works by first selecting a specified number of centroids (initial cluster centers) and assigning each data point to the nearest centroid. Then, the centroids are updated by computing the mean of all data points assigned to each cluster. This process is repeated iteratively until the centroids stabilize (no longer change significantly) or the maximum number of iterations is reached. K-Means is widely used for tasks like customer segmentation, anomaly detection, and pattern recognition.

Before implementing the K-Means Machine Learning algorithm, we need to consider using **Principal Component Analysis** in the context of this question.

**Principal Component Analysis** (PCA) is necessary in this case because the dataset contains a large number of features, which can lead to the curse of dimensionality in machine learning models like K-Means clustering. The curse of dimensionality refers to the phenomenon where the volume of the feature space increases exponentially with the number of dimensions (features), causing data points to become sparse. This sparsity makes it harder for algorithms to identify meaningful patterns or clusters.

PCA helps by reducing the dimensionality of the data while retaining the most important information. It does this by transforming the original features into a smaller set of uncorrelated variables, called principal components, that capture the maximum variance in the data. By reducing the number of features, PCA not only alleviates the curse of dimensionality but also improves the efficiency and accuracy of the K-Means algorithm by focusing on the most significant aspects of the data.
"""

# Import PCA from sklearn.decomposition
# Initialize PCA with 2 components to reduce the dimensionality of the data to 2 principal components
# random_state is set for reproducibility
pca = sklearn.decomposition.PCA(n_components=2, random_state=2)

# Apply PCA transformation to the data (data_c) and create a DataFrame with the transformed data
# The new columns are named 'pc1' and 'pc2', representing the two principal components
data_pca = pd.DataFrame(data = pca.fit_transform(data_c), columns = ['pc1', 'pc2'])

# Display the transformed data with the 2 principal components
data_pca

"""The code above applies Principal Component Analysis (PCA) to reduce the dimensionality of the dataset `data_c` to two principal components. First, it initializes a PCA object from `sklearn.decomposition` with `n_components=2`, specifying that the data should be transformed into two new features that capture the most variance. The `random_state=2` ensures that the results are reproducible. Next, it applies the `fit_transform()` method to the dataset, which computes the principal components and transforms the original data accordingly. The resulting two components are stored in a new DataFrame, data_pca, with columns labeled 'pc1' and 'pc2'. This reduced data can now be used for further analysis, such as clustering, with fewer features while retaining the most significant patterns in the data.

--------------------------------------------------------------------------------

#### Step 3: Obtaining A Suitable Number Of Clusters For The Model

###### Constructing The Elbow Plot

To identify the best number of clusters to explain our data, we visualise the decrease in SSE over the number of clusters, using an **elbow plot**.

**SSE (Sum of Squared Errors):**

SSE, or Sum of Squared Errors, is a metric used to measure the total variation within a dataset that is not explained by the clustering model. In the context of K-Means clustering, SSE is calculated by summing the squared distances between each data point and its assigned cluster centroid. A lower SSE indicates that the data points are closer to their centroids, implying that the clusters are well-defined and tightly grouped. Conversely, a higher SSE suggests that the points are more spread out from their centroids, indicating poorly defined clusters. SSE is an important evaluation metric in clustering because it helps quantify how well the model has fitted the data.

**Elbow Plot:**

An elbow plot is a graphical tool used to determine the optimal number of clusters for a K-Means clustering model. The plot displays the relationship between the number of clusters (x-axis) and the SSE (y-axis). As the number of clusters increases, SSE typically decreases because the data is divided into more groups, reducing the error within each cluster. However, after a certain point, adding more clusters results in only marginal reductions in SSE. The "elbow" in the plot refers to the point where the rate of decrease in SSE slows down significantly, indicating that adding more clusters beyond this point does not offer substantial improvements. This point is considered the optimal number of clusters, balancing model simplicity and performance.
"""

# Initialize an empty dictionary to store the SSE values for each number of clusters
sse = {}

# Loop over the range of cluster numbers from 1 to 14 to compute the SSE for each case
for k in range(1, 15):
    # Initialize and fit the KMeans model with k clusters, max iterations set to 10, and a random state for reproducibility
    kmeans = sklearn.cluster.KMeans(n_clusters=k, max_iter=10, random_state = k).fit(data_pca)

    # Assign the cluster labels to the data (the predicted clusters for each data point)
    data_pca["clusters"] = kmeans.labels_

    # Store the inertia (SSE) for the current number of clusters in the sse dictionary
    # Inertia represents the sum of squared distances of samples to their nearest cluster center
    sse[k] = kmeans.inertia_

# Plot the SSE values against the number of clusters to create the elbow plot
plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.xlabel("Number of clusters")
plt.ylabel("SSE")
plt.title("Elbow Plot: SSE vs Number of Clusters")
plt.show()

"""The elbow plot shows the Sum of Squared Errors (SSE) against the number of clusters, helping to determine the optimal number of clusters for K-Means clustering.

Key Insights And Analysis:

1. **Steep Decline (1 to 4 Clusters)**

The SSE drops sharply, indicating that adding clusters significantly reduces the error. This suggests that these clusters are meaningful and improve segmentation.

2. **Gradual Reduction (5 to 8 Clusters)**

The rate of decline slows down, indicating diminishing returns in error reduction. Clusters beyond this range still improve SSE but not as significantly.

3. **Minimal Improvement (9 Clusters or more)**

The curve flattens, meaning adding more clusters does not significantly reduce SSE. This suggests over-segmentation, where clusters may no longer be meaningful.

**Recommended Range:**

The optimal range of clusters appears to be **between 3 and 8**, where we balance reducing SSE while avoiding unnecessary complexity. We should avoid the minimal improvement range.

--------------------------------------------------------------------------------

###### Drawing The Cluster Diagrams

The code performs K-means clustering on a dataset projected into two principal components (`data_pca`) for a range of cluster values (k = 3 to 8) and visualizes the results in a 2-row, 3-column grid. It initializes a figure with six subplots using `plt.subplots()`, flattens the axes array for easy iteration, and then iterates over the chosen k values. For each k, it fits a K-means model, assigns clusters, and plots the data points with colors representing clusters. The cluster centroids are highlighted in red. Each subplot is labeled with titles, axes labels, and legends for clarity.
"""

# Initialize a range of k values (number of clusters to test)
k_range = range(3, 9)

# Create a figure with a 2-row, 3-column layout to display six graphs
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.flatten()  # Flatten the 2D array of axes to a 1D array for easy iteration

# Fit and plot data for each k value
for idx, k in enumerate(k_range):
    kmeans = sklearn.cluster.KMeans(n_clusters=k, random_state=2)
    clusters = kmeans.fit_predict(data_pca)  # Apply K-means clustering

    # Plot the clustered data points
    axes[idx].scatter(data_pca.pc1, data_pca.pc2, c=clusters, cmap='viridis', marker='o', edgecolor='k', s=100)

    # Plot cluster centroids
    axes[idx].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
                      s=300, c='red', label='Centroids', edgecolor='k')

    # Set plot title and labels
    axes[idx].set_title(f'K-means Clustering (k={k})')
    axes[idx].set_xlabel('Feature 1')
    axes[idx].set_ylabel('Feature 2')
    axes[idx].legend()
    axes[idx].grid()

# Adjust layout for better spacing
plt.tight_layout()
plt.show()

"""From the six plotted graphs, we observe that as the number of clusters (red dots) increases, cluster separation improves. When the number of clusters is set to 3 or 4, the data points are not well differentiated by color, indicating potential underfitting in the K-Means model. With 6 clusters, some turquoise data points remain close to the blue or purple clusters, which may lead to classification inaccuracies. However, this issue is mitigated with 7 and 8 clusters, where the separation becomes more distinct. To achieve an optimal balance between model accuracy and complexity, **selecting 7 clusters** is the most suitable choice for the Machine Learning model implementation.

--------------------------------------------------------------------------------

#### Step 4: K-Means Model Implementation

Now that the optimal number of clusters has been determined, we can proceed with using the K-Means model to assign clusters to our data points. K-Means was selected for guest segmentation due to its lower computational cost compared to hierarchical clustering or DBSCAN, making it a more efficient choice for handling large datasets while still providing effective segmentation.

Other Possible Methods:

**1. Hierarchical Clustering:**

Hierarchical clustering is a clustering algorithm that builds a tree-like structure (dendrogram) to group data points based on their similarities. It operates in two main approaches: agglomerative, which starts with individual data points and merges them iteratively, and divisive, which begins with all data points in one cluster and recursively splits them. This method does not require specifying the number of clusters in advance and provides a clear hierarchy of relationships between data points. However, it becomes computationally expensive as the dataset size increases, making it less suitable for large datasets.

**2. DBSCAN:**

DBSCAN is a clustering algorithm that groups data points based on density, making it effective for identifying clusters of varying shapes and sizes while also detecting noise (outliers). It works by defining core points (having a minimum number of neighbors within a given radius) and expanding clusters from these points. Unlike K-Means, DBSCAN does not require specifying the number of clusters and is robust to outliers. However, it struggles with datasets of varying densities and high-dimensional data, as selecting appropriate parameters (epsilon and minimum points) can be challenging.

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

**Why Hierarchical Clustering and DBSCAN Were Not Used:**

Hierarchical clustering was not chosen due to its high computational cost, especially for large datasets, making it impractical for efficient guest segmentation. While DBSCAN is useful for identifying clusters of arbitrary shapes and handling outliers, it requires fine-tuning of parameters and may not perform well when clusters have varying densities. In contrast, K-Means is computationally efficient, scales well with large datasets, and provides clear, well-separated clusters, making it the preferred choice for this application.
"""

# Initialize and fit K-Means with 7 clusters
kmeans = sklearn.cluster.KMeans(n_clusters=7, random_state=2)
clusters = kmeans.fit_predict(data_pca)  # Assign clusters to data points

# Set figure size
plt.figure(figsize=(8, 8))

# Create scatter plot for clustered data points
plt.scatter(data_pca.pc1, data_pca.pc2, c=clusters, cmap='viridis', marker='o', edgecolor='k', s=100)

# Plot cluster centroids
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            s=300, c='red', label='Centroids', edgecolor='k')

# Set plot title and labels
plt.title(f'K-means Clustering (k={7})')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.legend()
plt.grid()

# Display the plot
plt.show()

"""--------------------------------------------------------------------------------

#### Step 5: Observing Guest Segments And Identifying Characteristics

After finding the optimal number of clusters for the K-Means model, as well as visuzualizing the data and the clusters, we can now observe the guest segments and identify the characteristics of each segment by question type.

Question 1: Which type of theme park visitor best describes you?
"""

# Assign clusters to the original dataset
data_res = data_c.assign(cluster=sklearn.cluster.KMeans(n_clusters=7, random_state=2).fit_predict(data_pca))

# Filter columns that start with 'q1_'
q1_columns = [col for col in data_res.columns if col.startswith('q1_')]

# Display the mean of only the selected columns, grouped by cluster
print(data_res.groupby('cluster')[q1_columns].mean())

"""Key Insights:

Cluster 0: Has a relatively balanced distribution across different visitor types, with slightly higher proportions of families with teenagers (29%) and young children (26%), followed by visiting with friends (23%) and solo travelers (17%). This cluster likely represents a general attraction that appeals to diverse visitor groups.

Cluster 1: Strongly dominated by visitors with friends (78%), with minimal representation from other categories. This suggests attractions or destinations that are particularly popular for social group outings.

Cluster 2: Has a strong representation of families with young children (36%) and solo travelers (23%), along with families with teenagers (23%). This cluster might represent attractions that appeal to both family-oriented visitors and individual explorers.

Cluster 3: Predominantly families with young children (41%), with equal representation of families with teenagers and solo travelers (both 20%). This likely represents attractions with strong family appeal, particularly for those with younger children.

Cluster 4: Highest proportion of families with young children (47%) among all clusters, with families with teenagers and solo travelers each at 24%. This appears to be the most child-focused cluster, possibly representing destinations specifically designed for young families.

Cluster 5: Dominated by visitors with friends (66%), similar to Cluster 1 but with more solo travelers (11%) and families with teenagers (13%). This suggests social venues that may also accommodate some individual visitors.

Cluster 6: Very high proportion of families with young children (47%) with solo travelers (31%) being the second most common visitor type. This unusual combination might represent attractions that have specific appeal to both parents with young children and individual visitors.

--------------------------------------------------------------------------------

Question 2.1:	What is your age range?
"""

# Assign clusters to the original dataset
data_res = data_c.assign(cluster=sklearn.cluster.KMeans(n_clusters=7, random_state=2).fit_predict(data_pca))

# Filter columns that start with 'q1_'
q1_columns = [col for col in data_res.columns if col.startswith('q2_')]

# Display the mean of only the selected columns, grouped by cluster
print(data_res.groupby('cluster')[q1_columns].mean())

"""Key Insights:

Cluster 0: Has the highest proportion of teenagers (13-20 years) at 35%, with significant representation from young adults (21-34) at 21%. There's also a notable presence of children under 12 (19%). This suggests attractions that appeal to youth and young families.

Cluster 1: Overwhelmingly dominated by young adults (21-34) at 81%, with teenagers (13-20) making up most of the remainder at 17%. Almost no visitors from other age groups. This clearly represents venues or attractions specifically targeting young adults.

Cluster 2: Has a balanced distribution between teenagers (28%) and young adults (27%), with middle-aged adults (35-49) at 19% and children under 12 at 13%. This suggests attractions with broad appeal to younger generations and families.

Cluster 3: Shows the most even distribution across all age groups, with middle-aged adults (35-49) having the highest representation at 22%, followed by children under 12 (20%) and young adults (19%). This indicates attractions with universal appeal across generations.

Cluster 4: Features a very high proportion of children under 12 (53%) and teenagers (29%), with minimal representation from older age groups. This strongly suggests attractions specifically designed for children and youth.

Cluster 5: Heavily dominated by young adults (21-34) at 72%, with teenagers (13-20) at 21%. Almost no representation from older age groups. Similar to Cluster 1 but even more concentrated on young adults.

Cluster 6: Has the highest proportion of children under 12 (33%) and a significant percentage of middle-aged adults (35-49) at 24%. Teenagers (13-20) make up 22% of visitors. This suggests family-oriented attractions particularly appealing to families with children.

--------------------------------------------------------------------------------

Question 3: Are you a tourist or a local?
"""

# Assign clusters to the original dataset
data_res = data_c.assign(cluster=sklearn.cluster.KMeans(n_clusters=7, random_state=2).fit_predict(data_pca))

# Filter columns that start with 'q1_'
q1_columns = [col for col in data_res.columns if col.startswith('q3')]

# Display the mean of only the selected columns, grouped by cluster
print(data_res.groupby('cluster')[q1_columns].mean())

"""Key Insights:

Cluster 0: Predominantly local visitors (92%) with very few tourists (8%). This strongly suggests attractions that primarily serve the local community.

Cluster 1: Mostly local visitors (72%) but with a significant tourist component (28%). This represents attractions that primarily cater to locals but have some appeal to tourists as well.

Cluster 2: Exclusively tourists (100%) with no local visitors. This clearly indicates attractions that are specifically targeted at or primarily visited by tourists.

Cluster 3: Exclusively local visitors (100%) with no tourists. This represents attractions that are purely focused on serving the local community.

Cluster 4: Like Cluster 3, this is exclusively visited by locals (100%) with no tourists. This represents another category of attractions that solely serve local residents.

Cluster 5: Almost entirely local visitors (98%) with minimal tourist presence (2%). This suggests attractions that are very strongly oriented toward local community needs.

Cluster 6: Almost entirely tourists (98%) with minimal local presence (2%). Similar to Cluster 2, this represents attractions that are almost exclusively tourist destinations.

--------------------------------------------------------------------------------

Question 6: The type of attractions you enjoy the most when visiting USS
"""

# Assign clusters to the original dataset
data_res = data_c.assign(cluster=sklearn.cluster.KMeans(n_clusters=7, random_state=2).fit_predict(data_pca))

# Filter columns that start with 'q1_'
q1_columns = [col for col in data_res.columns if col.startswith('q6')]

# Display the mean of only the selected columns, grouped by cluster
print(data_res.groupby('cluster')[q1_columns].mean())

"""Key Insights:

Cluster 0: Shows a balanced interest across multiple attraction types with strongest preferences for 3D/4D experiences (46%), souvenir shops (48%), water rides (44%), and performances (38%). This suggests visitors who enjoy a diverse theme park experience with both thrills and entertainment.

Cluster 1: Very strong preference for souvenir shops (89%) and 3D/4D experiences (65%), with roller coasters (67%) also being popular. This indicates visitors who prioritize shopping and specialty attractions.

Cluster 2: Shows relatively balanced preferences across all attraction types, with souvenir shops (42%), water rides (42%), and 3D/4D experiences (37%) being slightly more popular. This suggests visitors who enjoy a variety of experiences.

Cluster 3: Highest interest in eateries/restaurants (45%) and other rides (41%), with roadshows (39%) also being popular. This cluster seems to represent visitors who prioritize food and more conventional attractions.

Cluster 4: Strong preference for roadshows (65%) and other rides (53%), with notably low interest in souvenir shops (12%) and water rides (6%). This suggests visitors focused on shows and standard rides rather than shopping or water attractions.

Cluster 5: Overwhelmingly interested in souvenir shops (81%) and eateries/restaurants (40%), with minimal interest in roadshows (9%). This clearly represents visitors primarily focused on shopping and dining.

Cluster 6: Highest preference for roadshows (52%) and other rides (41%), with relatively low interest in souvenir shops (16%) and water rides (10%). Similar to Cluster 4, this suggests visitors who prioritize shows and standard rides.

--------------------------------------------------------------------------------

Question 7: Factors that will influence your decision to visit a theme park like USS?
"""

# Assign clusters to the original dataset
data_res = data_c.assign(cluster=sklearn.cluster.KMeans(n_clusters=7, random_state=2).fit_predict(data_pca))

# Filter columns that start with 'q1_'
q1_columns = [col for col in data_res.columns if col.startswith('q7')]

# Display the mean of only the selected columns, grouped by cluster
print(data_res.groupby('cluster')[q1_columns].mean())

"""Key Insights:

Cluster 0: Key decision factors include holiday seasons (46%), wait times for rides (42%), safety and cleanliness (41%), and attraction variety (40%). This suggests visitors who prioritize both practical considerations and seasonal opportunities.

Cluster 1: Strongly influenced by cost and ticket prices (85%), weather conditions (81%), and wait times (65%). Holiday seasons (57%) and safety (56%) are also important. This indicates price-sensitive visitors who are highly concerned with comfort and convenience.

Cluster 2: Shows relatively balanced consideration across factors, with cost (39%), weather and holiday seasons (both 35%) having slight priority. These visitors seem to make decisions based on a balanced mix of practical factors.

Cluster 3: Most influenced by weather conditions (38%), safety and cleanliness (34%), with fairly even consideration of other factors. This suggests visitors who prioritize comfort and security.

Cluster 4: Primarily concerned with special events (59%) and holiday seasons (47%), with location (35%) being the third most important factor. This indicates visitors who plan their trips around specific events or seasonal offerings.

Cluster 5: Strongly influenced by cost and ticket prices (79%), weather conditions (72%), and wait times (64%). Similar to Cluster 1, these are price-sensitive visitors concerned with comfort and efficiency.

Cluster 6: Most heavily influenced by special events (50%) and location/accessibility (43%), with wait times (36%) also being significant. This suggests visitors who make decisions based on convenience and special programming.

--------------------------------------------------------------------------------

Question 10: When do you usually visit theme parks or attractions like USS?
"""

# Assign clusters to the original dataset
data_res = data_c.assign(cluster=sklearn.cluster.KMeans(n_clusters=7, random_state=2).fit_predict(data_pca))

# Filter columns that start with 'q1_'
q1_columns = [col for col in data_res.columns if col.startswith('q10')]

# Display the mean of only the selected columns, grouped by cluster
print(data_res.groupby('cluster')[q1_columns].mean())

"""Key Insights:

Cluster 0: Strong preference for weekdays (57%) and school holidays (53%), with special events (50%) also being significant. This suggests visitors who avoid peak weekend times and prefer to visit during school breaks or for special programming.

Cluster 1: Balanced preferences for school holidays (54%), weekdays (46%), and weekends (44%), with special events (43%) also being important. This indicates visitors with flexible scheduling who visit across different time periods.

Cluster 2: Prefer weekends (49%) and special events (50%), with public holidays (47%) and school holidays (46%) also being significant. This suggests visitors who prioritize non-work days and special programming.

Cluster 3: Strongest preference for public holidays (57%), with weekends (54%) and school holidays (45%) also being important. This indicates visitors who primarily come during holiday periods.

Cluster 4: Very strong preference for public holidays (65%) and weekdays (59%), with school holidays (53%) also being significant. This suggests a unique visitor segment that strongly prefers official holidays and regular weekdays.

Cluster 5: Most likely to visit during school holidays (49%) and weekdays (49%), with lower preferences for special events (36%) and weekends (26%). This indicates visitors who prioritize less crowded periods.

Cluster 6: Strong preference for weekdays (59%), with public holidays (50%) and school holidays (47%) also being important. This indicates visitors who prioritize weekdays and holiday periods.

--------------------------------------------------------------------------------

Question 12: How do you usually navigate a theme park like USS?
"""

# Assign clusters to the original dataset
data_res = data_c.assign(cluster=sklearn.cluster.KMeans(n_clusters=7, random_state=2).fit_predict(data_pca))

# Filter columns that start with 'q1_'
q1_columns = [col for col in data_res.columns if col.startswith('q12')]

# Display the mean of only the selected columns, grouped by cluster
print(data_res.groupby('cluster')[q1_columns].mean())

"""Key Insights:

Cluster 0: Strong preference for following the shortest queue (54%), with balanced secondary preferences for pre-planned routes (23%) and spontaneous exploration (22%). This indicates visitors who prioritize efficiency and minimizing wait times.

Cluster 1: Overwhelmingly prefers spontaneous exploration (87%), with minimal planning or queue optimization. This suggests visitors who value discovery and flexibility over efficiency.

Cluster 2: Prefers spontaneous exploration (55%), with pre-planned routes (27%) being the second choice. This indicates visitors who enjoy discovery but with some structure.

Cluster 3: Very strong preference for spontaneous exploration (86%), similar to Cluster 1, with minimal interest in following queues or planning routes. This suggests another segment of discovery-oriented visitors.

Cluster 4: Unique in having equal preferences for following shortest queues and pre-planned routes (both 47%), with very little interest in spontaneous exploration (6%). This indicates highly organized visitors who prioritize efficiency and structure.

Cluster 5: Balanced preferences between spontaneous exploration (43%) and following shortest queues (40%), with less interest in pre-planned routes (17%). This suggests visitors who balance discovery with practical queue management.

Cluster 6: Prefers spontaneous exploration (48%), with pre-planned routes (28%) and following shortest queues (24%) being secondary considerations. This indicates visitors who prioritize discovery but still consider efficiency factors.

--------------------------------------------------------------------------------

### Exporting The Data To Google Drive

Lastly, we can export the dataset as well as the clusters outcome back to Google Drive as future questions require the cluster dataset for analysis.
"""

# CLUSTERS + ORIGINAL DATASET

# Perform PCA and then apply KMeans clustering with 7 clusters
data_res = data_clean.assign(cluster = sklearn.cluster.KMeans(n_clusters=7, random_state=2).fit_predict(data_pca))

# Combine the original dataset (df) with the cluster labels
data_final = pd.concat([data_res.cluster, df], axis=1)

# Display the first few rows of the final dataset (with clusters assigned)
print(data_final.head())

# Save a copy of the clustered data into Google Drive
data_final.to_csv('/content/drive/MyDrive/dsa3101_clustered_data.csv', index=True)

"""****************************************************************************************************************************************************************"""