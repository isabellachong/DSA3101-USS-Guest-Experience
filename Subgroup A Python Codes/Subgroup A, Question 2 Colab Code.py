# -*- coding: utf-8 -*-
"""Subgroup A Question 2 Google Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Q333PlLOeEEtf5WakqMNCzF8QtMm0O2

## Importing From Google Drive
"""

''' ignore this part, as it appears that google.colab module can only be run on google colab itself
# Importing From Google Drive
from google.colab import drive
drive.mount('/content/drive')
'''

"""## Importing The Necessary Packages"""

# Importing The Necessary Packages

# 1. pandas - to be used for data cleaning
import pandas as pd

# 2. numpy - to be used for numerical computing
import numpy as np

# 3. matplotlib - to be used for data visualizations
import matplotlib.pyplot as plt

# 4. seaborn - to be used for data visualizations
import seaborn as sns

# 5. sklearn - to be used for Machine Learning implementation
from sklearn import preprocessing, decomposition, cluster

"""## Other Settings Implemented Using Pandas

This is an optional step but the following block of code below helps to change the output structure of the code such that

**1) All the columns of the dataset will be printed**

**2) The width of the dataset output is not limited to the display width of Google Colab**

**3) Prevent wrapping the output to multiple lines on Google Colab to improve readibility**
"""

# Show all columns of the dataset when printed
pd.set_option('display.max_columns', None)

# Don't limit the display width of the output
pd.set_option('display.width', None)

# Don't wrap the output to multiple lines
pd.set_option('display.expand_frame_repr', False)

"""## Reading The Excel File From Google Drive To Google Colab"""

# NOTE: this section is different from google colab
df = pd.read_excel("uss_survey_responses.xlsx")

"""## Examining The Number Of Rows And Columns Of The Dataset

We can examine the number of rows and columns of the dataset using `df.shape`, where the first number represents the number of rows and the second number represents the number of columns of the dataset.
"""

# Finding the number of rows and columns of the dataset
num_rows, num_columns = df.shape

# Displaying the results
print("Number of Rows:", num_rows)
print("Number of Columns:", num_columns)

"""We observe that there are 505 rows and 56 columns in the dataset.
The columns include the email address of the survey responders, as well as their responses to the 20 questions in the survey (some of the survey questions have various subparts, hence more than 20 columns altogether). We currently have 505 survey responses in our dataset.

Since the email address is considered highly confidential, in order to maintain data integrity to prevent exposure of information and privacy leaks, we should remove the `email address` column. Also, the `time_entry` column is not really important in our analysis as this column just represents when the respondants have completed the survey (within a period of a few days, all recent entries). We can also remove the column.
"""

# Removing the email addressand time_entry column of the dataset
df = df.drop('Email Address', axis = 1)
df = df.drop('time_entry', axis = 1)

# Displaying the first few rows of the updated dataset
print(df.head())

"""## What Each Column Of The Dataset Represent (From The Survey Questions)

Here is a description of the what each of the various columns of the dataset represent:

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        table {
            width: 50%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid black;
            padding: 10px;
            text-align: center;
        }
    </style>
</head>
<body>
    <table>
        <thead>
            <tr>
                <th>Column Name</th>
                <th>Description Of Column</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>q1</td>
                <td>Which type of theme park visitor best describes you?</td>
            </tr>
            <tr>
                <td>q2_1</td>
                <td>What is your age range?</td>
            </tr>
            <tr>
                <td>q2_2</td>
                <td>What is your gender?</td>
            </tr>
            <tr>
                <td>q3</td>
                <td>Are you a tourist or a local?</td>
            </tr>
            <tr>
                <td>q4_1</td>
                <td>For the category on thrill rides, what is the average time you queued for?</td>
            </tr>
            <tr>
                <td>q4_2</td>
                <td>For the category on interactive exhibits, what is the average time you queued for?</td>
            </tr>
            <tr>
                <td>q4_3</td>
                <td>For the category on performances, what is the average time you queued for?</td>
            </tr>
            <tr>
                <td>q4_4</td>
                <td>For the category on food and dining, what is the average time you queued for?</td>
            </tr>
            <tr>
                <td>q5_1</td>
                <td>For the category on thrill rides, what is considered an unacceptable wait time? (Integer in Minutes)</td>
            </tr>
            <tr>
                <td>q5_2</td>
                <td>For the category on interactive exhibits, what is considered an unacceptable wait time? (Integer in Minutes)</td>
            </tr>
            <tr>
                <td>q5_3</td>
                <td>For the category on performances, what is considered an unacceptable wait time? (Integer in Minutes)</td>
            </tr>
            <tr>
                <td>q5_4</td>
                <td>For the category on food and dining, what is considered an unacceptable wait time? (Integer in Minutes)</td>
            </tr>
            <tr>
                <td>q6</td>
                <td>The type of attractions you enjoy the most when visiting USS</td>
            </tr>
            <tr>
                <td>q7</td>
                <td>Factors that will influence your decision to visit a theme park like USS?</td>
            </tr>
            <tr>
                <td>q8</td>
                <td>What type of events influence your decision to visit?</td>
            </tr>
            <tr>
                <td>q9</td>
                <td>How long do you usually spend at USS? (Integer in Hours)</td>
            </tr>
            <tr>
                <td>q10</td>
                <td>When do you usually visit theme parks or attractions like USS?</td>
            </tr>
            <tr>
                <td>q11</td>
                <td>When do you typically purchase meals or snacks at the eateries/restaurants?</td>
            </tr>
            <tr>
                <td>q12</td>
                <td>How do you usually navigate a theme park like USS?</td>
            </tr>
            <tr>
                <td>q13</td>
                <td>Would you be willing to wear a digital watch given by USS to track your location and activity?</td>
            </tr>
            <tr>
                <td>q14_1</td>
                <td>At what time of the day do you usually visit roller coasters?</td>
            </tr>
            <tr>
                <td>q14_2</td>
                <td>At what time of the day do you usually visit water rides?</td>
            </tr>
            <tr>
                <td>q14_3</td>
                <td>At what time of the day do you usually visit 3D/4D experiences?</td>
            </tr>
            <tr>
                <td>q14_4</td>
                <td>At what time of the day do you usually visit performances?</td>
            </tr>
            <tr>
                <td>q14_5</td>
                <td>At what time of the day do you usually visit roadshows?</td>
            </tr>
            <tr>
                <td>q14_6</td>
                <td>At what time of the day do you usually visit eateries and restaurants?</td>
            </tr>
            <tr>
                <td>q14_7</td>
                <td>At what time of the day do you usually visit souvenir shops?</td>
            </tr>
            <tr>
                <td>q14_8</td>
                <td>At what time of the day do you usually visit other rides (carousel rides, teacup rides etc.)?</td>
            </tr>
            <tr>
                <td>q15</td>
                <td>How likely are you to recommend USS to others?</td>
            </tr>
            <tr>
                <td>q16_1</td>
                <td>How satisfied are you with the overall service of the queuing system?</td>
            </tr>
            <tr>
                <td>q16_2</td>
                <td>How satisfied are you with the overall service of retail experience?</td>
            </tr>
            <tr>
                <td>q16_3</td>
                <td>How satisfied are you with the overall service of eateries/restaurants?</td>
            </tr>
            <tr>
                <td>q16_4</td>
                <td>How satisfied are you with the overall service of photo taking exhibitions?</td>
            </tr>
            <tr>
                <td>q16_5</td>
                <td>How satisfied are you with the overall service of entertainment attractions?</td>
            </tr>
            <tr>
                <td>q17_1</td>
                <td>Give an overall rating for ticketing information accessibility</td>
            </tr>
            <tr>
                <td>q17_2</td>
                <td>Give an overall rating for rides and attractions</td>
            </tr>
            <tr>
                <td>q17_3</td>
                <td>Give an overall rating for entertainment and performances</td>
            </tr>
            <tr>
                <td>q17_4</td>
                <td>Give an overall rating for food and beverage</td>
            </tr>
            <tr>
                <td>q17_5</td>
                <td>Give an overall rating for merchandise and shopping</td>
            </tr>
            <tr>
                <td>q17_6</td>
                <td>Provide an overall rating for crowd management, comfort and staff helpfulness</td>
            </tr>
            <tr>
                <td>q18_1</td>
                <td>For ticketing information accessibility, which of the following services are you not satisfied with?</td>
            </tr>
            <tr>
                <td>q18_2</td>
                <td>For rides and attractions, which of the following services are you not satisfied with?</td>
            </tr>
            <tr>
                <td>q18_3</td>
                <td>For entertainment and performances, which of the following services are you not satisfied with?</td>
            </tr>
            <tr>
                <td>q18_4</td>
                <td>For food and beverage, which of the following services are you not satisfied with?</td>
            </tr>
            <tr>
                <td>q18_5</td>
                <td>For merchandise and shopping, which of the following services are you not satisfied with?</td>
            </tr>
            <tr>
                <td>q18_6</td>
                <td>For crowd management, comfort and staff helpfulness, which of the following services are you not satisfied with?</td>
            </tr>
            <tr>
                <td>q19_1</td>
                <td>How important is ticketing information accessibility to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q19_2</td>
                <td>How important is crowd management to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q19_3</td>
                <td>How important is staff helpfulness to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q19_4</td>
                <td>How important is safety and cleanliness to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q19_5</td>
                <td>How important is rides and attractions to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q19_6</td>
                <td>How important is food and beverage to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q19_7</td>
                <td>How important is merchandise and shopping to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q19_8</td>
                <td>How important is entertainment and performances to your overall satisfaction?</td>
            </tr>
            <tr>
                <td>q20</td>
                <td>Is there any other feedback about your USS experience that you want to mention?</td>
            </tr>
        </tbody>
    </table>
</body>
</html>

****************************************************************************************************************************************************************

### Business Question 2: Creating a Guest Segmentation Model

#### Overview

Now that we have a good overview of the key factors, let's investigate the data at a deeper level, to get a better idea of our guests. Here is a brief overview of the guest segmentation process via **k-means clustering**.  
  
Step 1: **Feature engineering & Data Cleaning** to retain the most insightful rows and scale data, in preparation for later steps.

Step 2: **Principle Compoment Analysis(PCA)** for dimensionality reduction.

Step 3: **Parameter tuning** for model, using elbow method and other visualisation methods to identify the ideal number of clusters.

Step 4: **K-means clustering** model implementation to label the different clusters.

Step 5: **Analysis of various guest segments** and their key distinctions and characteristics.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### Step 1: Feature engineering & Data Cleaing

##### Section A: Selecting important rows

Features like q18_1 - q18_6 have been removed as they are too detailed and may cause high variance in the various customer groups. Others like q8 overlaps with q7 and have thus been removed as well.
"""

# extracting the rows to be kept
data = df[[
    # demographics
    'q1', 'q2_1', 'q3',
    # unacceptable wait time
    'q5_1', 'q5_2', 'q5_3', 'q5_4',
    # factors
    'q6', 'q7',
    # time/activites in USS
    'q9', 'q10', 'q12',
    # reccomendation
    'q15',
    # services
    'q16_1', 'q16_2', 'q16_3', 'q16_4', 'q16_5',
    # importance
    'q19_1', 'q19_2', 'q19_3', 'q19_4', 'q19_5', 'q19_6', 'q19_7', 'q19_8']]

print(data.head())

len(data)

"""##### Section B: Data Cleaning

Now that we have selected the features we intend to use for our model, we will now proceed to clean the data, by **handling any NaN values** or **outliers** present.

######Handling NaN values

We observe that row 126 does not provide any information for any of the columns and row 119, has unusually huge numbers, which makes it an outlier. Hence they were both dropped.  
As for the remaining rows, we've decided to drop them as well, since inserting randomly generated numbers may compromise data accuracy.
"""

# observing null values in data
print('null values in data: \n', data[data.isna().any(axis=1)], '\n')

# dropping all rows with null values, and resetting index
data_clean = data.dropna()#.reset_index(drop=True)
print('removed null values: \n', data_clean.head())

len(data_clean)

"""###### Removing outliers

Taking a quick glance at the **numrical data**, we observe that the maximum wait times (q5_1 - q5_4) for each feature is reasonably large. Hence we decided to **keep these values** and maintain data integrity, even though the values are far from the upper quartile.
"""

# Note: columns q15 - q19_8 have been omitted as they exist within a fixed range.
numerical = ['q5_1', 'q5_2', 'q5_3', 'q5_4', 'q9']

print(data_clean[numerical].describe())

"""For the **categorical variables**, we first changed all labels to lower case to ensure data consistency. "Other Rides (Teacup Ride, Suspended Coasters, Carousel Rides)" and "Special Events (Halloween, Summer Festival, Christmas etc.)" for q6 and q10 respectively have also been **replaced with shorter names** for ease of encoding in Section C."""

# replacing q6 "Other Rides (Teacup Ride, Suspended Coasters, Carousel Rides)" to split the data by commas
pd.options.mode.chained_assignment = None
data_clean['q6'] = data_clean['q6'].str.replace(r'\(.*\)', '', regex=True)

# replacing q10 "Special Events (Halloween, Summer Festival, Christmas etc.)"
data_clean['q10'] = data_clean['q10'].str.replace(r'\(.*\)', '', regex=True)

# changing all values to lowercase
data_clean = data_clean.apply(lambda x: x.astype(str).str.lower())
data_clean

"""Observing for outliers, we find that there are no outliers for q10, but 2 for q7, which have been removed. We chose to remove them as they were very niche comments and a large majority chose the other options. It seems unlikely that these outliers form a cluster.  """

# outliers observed
# Note: q1 - q3, q6 and q11 were omitted as they had fixed options with no additional categories.
all_7 = set([x for sub in data_clean['q7'].apply(lambda x: [j.strip() for j in x.split(",")]) for x in sub])
out_7 = all_7 - set(['weather conditions', 'safety and cleanliness', 'attraction variety', 'cost and ticket prices', 'special events',
                     'reputation and reviews', 'holiday seasons', 'location and accessibility', 'wait times for rides'])
print(f'outliers for q7: {out_7}')

all_10 = set([x for sub in data_clean['q10'].apply(lambda x: [j.strip() for j in x.split(",")]) for x in sub])
out_10 = all_10 - set(['weather conditions', 'special events', 'public holidays', 'school holidays', 'weekends', 'weekdays'])
print(f'outliers for q10: {out_10}')

# removing outliers for q7
data_clean['q7'] = data_clean['q7'].str.replace(r', aesthetics', '', regex=True)
data_clean['q7'] = data_clean['q7'].str.replace(r', thrill factor \(.*\)', '', regex=True)

"""##### Section C: Scaling Numerical Features, Encoding Categorical Variables

Now that we have removed the outliers for both types of variables, we will now move on to encode the categorical variables and scale the numerical ones.
"""

numerical = ['q5_1', 'q5_2', 'q5_3', 'q5_4', 'q9', 'q15', 'q16_1', 'q16_2', 'q16_3', 'q16_4', 'q16_5',
             'q19_1', 'q19_2', 'q19_3', 'q19_4', 'q19_5', 'q19_6', 'q19_7', 'q19_8']

categorical = ['q1', 'q2_1', 'q3', 'q6', 'q7', 'q10', 'q12']

"""Encoding categorical variables using one-hot encoding:"""

mlb = preprocessing.MultiLabelBinarizer()
transformed_data = mlb.fit_transform(data_clean['q1'].apply(lambda x: ['q1_'+sec.strip() for sec in x.split(",")]))
data_encode_cat = pd.DataFrame(transformed_data, columns = mlb.classes_, index = data_clean.index)

mlb = {}

for c in categorical[1:]:
    mlb[c] = preprocessing.MultiLabelBinarizer()

    temp = pd.DataFrame(mlb[c].fit_transform(data_clean[c].apply(lambda x: [c+'_'+sec.strip() for sec in x.split(",")])),
                       columns=mlb[c].classes_,
                       index=data_clean[c].index)
    data_encode_cat = pd.concat([data_encode_cat, temp], axis=1)

# column produced by a typo in the file. manually removed as data accuracy remains unaffected
#data_encode_cat = data_encode_cat.drop('q11_', axis=1)

data_encode_cat

"""Scaling the numerical columns using MinMaxScaler(). This was chosen instead of StandardScaler(), because as seen in Step 1 Section B, distribution is uncertain, especially for queue times."""

data_scale_num = data_clean[numerical]
scaler = preprocessing.MinMaxScaler()
data_scale_num[:] = scaler.fit_transform(data_scale_num)
print(data_scale_num)

"""merging the two dataframes together"""

data_c = pd.concat([data_encode_cat, data_scale_num], axis=1)
data_c

"""#### Step 2: PCA

This step is necessary as even after carefully selecting columns, the number of features present is still too great. This can cause **curse of dimensionality** for the chosen K-means model. Hence, we have reduced the data into 2 PCA columns as shown below.
"""

pca = decomposition.PCA(n_components=2, random_state=2)
data_pca = pd.DataFrame(data = pca.fit_transform(data_c), columns = ['pc1', 'pc2'])

data_pca

"""#### Step 3: find suitable number of clusters to set

To identify the best number of clusters to explain our data, we visualise the decrease in SSE over the number of clusters, using an **elbow plot** as shown below.

As observed, it appears that **2-8 groups** are optimal, but more visualisation is necessary to identify the best number of clusters.
"""

sse = {}
for k in range(1, 15):
    kmeans = cluster.KMeans(n_clusters=k, max_iter=10, random_state = k).fit(data_pca)
    data_pca["clusters"] = kmeans.labels_
    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center
plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.xlabel("Number of cluster")
plt.ylabel("SSE")
plt.show()

"""For each count of clusters, we have plotted a graph for the clusters with colour labels and centroids. Here we see that **7 clusters** is the minimum number that best explains the data."""

# Initialize a range of k values
k_range = range(3, 9)

# Fit and plot data for each k value
for k in k_range:
    kmeans = cluster.KMeans(n_clusters=k, random_state=2)
    clusters = kmeans.fit_predict(data_pca)

    # Plot the clustered data points
    plt.scatter(data_pca.pc1, data_pca.pc2, c=clusters, cmap='viridis', marker='o', edgecolor='k', s=100)
    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
                s=300, c='red', label='Centroids', edgecolor='k')
    plt.title(f'K-means Clustering (k={k})')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.grid()
    plt.show()

"""#### Step 4: model implementation

Now that we know the optimal number of clusters, we can now use the k-means model to assign the clusters to our data points. K-means was chosen as the model for guest segmentation, as it is less computationally costlys as compared to hierarchical clustering or DBSCAN.   
  
*Note: when the model was ran using DBSCAN, it overfitted the data wuth 13 clusters.*  

Here is a plot of our final gets segments.
"""

kmeans = cluster.KMeans(n_clusters=7, random_state=2)
clusters = kmeans.fit_predict(data_pca)

# Plot the clustered data points
plt.scatter(data_pca.pc1, data_pca.pc2, c=clusters, cmap='viridis', marker='o', edgecolor='k', s=100)
plt.scatter(data_pca.pc1, data_pca.pc2, c=clusters, cmap='viridis', marker='o', edgecolor='k', s=100)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', label='Centroids', edgecolor='k')
plt.title(f'K-means Clustering (k={k})')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.legend()
plt.grid()
plt.show()

"""#### Step 5: identifying guest segments

Observing the graph we find that the data can be best categorised into 7 key clusters. Let's split the data into those clusters and observe their traits.

Assigning the cleaned data to its respective cluster:
"""

# CLUSTERS + CLEANED DATA (SCALED)
data_res = data_c.assign(cluster = cluster.KMeans(n_clusters=7, random_state=2).fit_predict(data_pca))
print(data_res.head())
print(data_res.groupby('cluster').mean())

"""##### Visualising the different clusters
credits to brenda's plots
with modifications
"""

# categorical data
def plot_bar_per_cluster(df, question, cluster_col='cluster'):
    clusters = sorted(df[cluster_col].dropna().unique())
    all_labels = df.columns[pd.Series(df.columns).str.startswith(question)]
    plot_data = pd.DataFrame(index=all_labels)

    for c in clusters:
        responses = df[df[cluster_col] == c][all_labels]
        responses = responses.sum()/responses.values.sum()
        plot_data[f'Cluster {c}'] = responses

    plot_data.plot(kind='bar', figsize=(12, 6))
    plt.title(f"Distribution of '{question}' Responses by Cluster")
    plt.xlabel("Responses")
    plt.ylabel("Count")
    plt.xticks(rotation=45)
    plt.legend(title="Cluster")
    plt.tight_layout()
    plt.show()

plot_bar_per_cluster(data_res, 'q1_')
plot_bar_per_cluster(data_res, 'q2_')
plot_bar_per_cluster(data_res, 'q3_')
plot_bar_per_cluster(data_res, 'q6_')
plot_bar_per_cluster(data_res, 'q7_')
plot_bar_per_cluster(data_res, 'q10_')
plot_bar_per_cluster(data_res, 'q12_')

# numerical data
def plot_bar_per_cluster(df, question, cluster_col='cluster'):
    clusters = sorted(df[cluster_col].dropna().unique())
    all_labels = df.columns[pd.Series(df.columns).str.startswith(question)]
    plot_data = pd.DataFrame(index=all_labels)

    for c in clusters:
        responses = df[df[cluster_col] == c][all_labels]
        responses = responses.mean()
        plot_data[f'Cluster {c}'] = responses

    plot_data.plot(kind='bar', figsize=(12, 6))
    plt.title(f"Distribution of '{question}' Responses by Cluster")
    plt.xlabel("Responses")
    plt.ylabel("Count")
    plt.xticks(rotation=45)
    plt.legend(title="Cluster")
    plt.tight_layout()
    plt.show()

plot_bar_per_cluster(data_res, 'q5')
plot_bar_per_cluster(data_res, 'q9')
plot_bar_per_cluster(data_res, 'q15')
plot_bar_per_cluster(data_res, 'q16')
plot_bar_per_cluster(data_res, 'q19')

"""##### Final clusters identified

Observing how each of the clusters fare across different characteristics, here are some of the key traits we observe:

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        table {
            width: 50%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid black;
            padding: 10px;
            text-align: center;
        }
    </style>
</head>
<body>
    <table>
        <thead>
            <tr>
                <th>Cluster</th>
                <th>Name</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>0</td>
                <td>Young Tourists  (13-34 years old)</td>
                <td>Most of them travel with families, and generally try all features in a spontaneous manner.</td>
            </tr>
            <tr>
                <td>1</td>
                <td>Leisure Young Locals (13-34 years old)</td>
                <td>This cluster behaves similar to cluster 3, but are more attracted by special events.</td>
            </tr>
            <tr>
                <td>2</td>
                <td>Thrill-seeking Young Adult Locals (21-34 years old)</td>
                <td>This cluster generally enjoys on exciting rides (e.g. roller coasters/water rides), and visits with friends.</td>
            </tr>
            <tr>
                <td>3</td>
                <td>Experiential Locals (0-34 years old)</td>
                <td>These families visit for a good time and memories. They favour photobooths and performances but prioritise satisfaction across all features.</td>
            </tr>
                <td>4</td>
                <td>Time optimisers (13-20 years old)</td>
                <td>This group generally visit on weekdays, staying longer than most. They like shorter queues, and plan routes in advance.</td>
            </tr>
            <tr>
                <td>5</td>
                <td>Young Families (0-20, 35-49 years old)</td>
                <td>Most visit roadshows, surprisingly disatisfied, visits with their family on school and public holidays and spends least time at USS.</td>
            </tr>
            <tr>
                <td>6</td>
                <td>Experiential Tourist Families (0-20, 35-49 years old)</td>
                <td>This cluster like cluster 1, visits all the features, but focus most on souvenir shops and convenient locations.</td>
            </tr>
        </tbody>
    </table>
</body>
</html>

### Exporting Data
"""

# CLUSTERS + ORIGINAL DATASET
data_res = data_clean.assign(cluster = cluster.KMeans(n_clusters=7, random_state=2).fit_predict(data_pca))
data_final = pd.concat([data_res.cluster, df], axis=1)
print(data_final.head())

# save a copy of clustered data onto google drive
data_final.to_csv('dsa3101_clustered_data.csv', index=True)

"""****************************************************************************************************************************************************************"""
