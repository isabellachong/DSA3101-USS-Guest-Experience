{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import time\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data from CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zone: Sci-Fi City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accelerator\n",
    "file_path = \"/Users/cheryl/Downloads/DSA3101/data/Accelerator\"\n",
    "directory = os.fsencode(file_path)\n",
    "all_files = Path(file_path).glob('*.csv')\n",
    "\n",
    "# Concatenate it\n",
    "accelerator = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Zone'] = 'Sci-Fi City'\n",
    "    accelerator = pd.concat([accelerator, df], ignore_index=True)\n",
    "\n",
    "\n",
    "## Battlestar Galactica CYCLON\n",
    "file_path = \"/Users/cheryl/Downloads/DSA3101/data/Battlestar Galactica CYCLON\"\n",
    "directory = os.fsencode(file_path)\n",
    "all_files = Path(file_path).glob('*.csv')\n",
    "\n",
    "# Concatenate it\n",
    "cyclon = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Zone'] = 'Sci-Fi City'\n",
    "    cyclon = pd.concat([cyclon, df], ignore_index=True)\n",
    "\n",
    "\n",
    "## Battlestar Galactica HUMAN\n",
    "file_path = \"/Users/cheryl/Downloads/DSA3101/data/Battlestar Galactica HUMAN\"\n",
    "directory = os.fsencode(file_path)\n",
    "all_files = Path(file_path).glob('*.csv')\n",
    "\n",
    "# Concatenate it\n",
    "human = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Zone'] = 'Sci-Fi City'\n",
    "    human = pd.concat([human, df], ignore_index=True)\n",
    "\n",
    "\n",
    "## TRANSFORMERS The Ride\n",
    "file_path = \"/Users/cheryl/Downloads/DSA3101/data/TRANSFORMERS The Ride\"\n",
    "directory = os.fsencode(file_path)\n",
    "all_files = Path(file_path).glob('*.csv')\n",
    "\n",
    "# Concatenate it\n",
    "transformers = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Zone'] = 'Sci-Fi City'\n",
    "    transformers = pd.concat([transformers, df], ignore_index=True)\n",
    "\n",
    "\n",
    "## Concatenate Sc-Fi City dataframes\n",
    "sci_fi_city = pd.concat([accelerator, cyclon, human, transformers], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zone: The Lost World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Canopy Flyer\n",
    "file_path = \"/Users/cheryl/Downloads/DSA3101/data/Canopy Flyer\"\n",
    "directory = os.fsencode(file_path)\n",
    "all_files = Path(file_path).glob('*.csv')\n",
    "\n",
    "# Concatenate it\n",
    "canopy = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Zone'] = 'The Lost World'\n",
    "    canopy = pd.concat([canopy, df], ignore_index=True)\n",
    "\n",
    "\n",
    "## Dino-soarin'\n",
    "file_path = \"/Users/cheryl/Downloads/DSA3101/data/Dino-soarin\"\n",
    "directory = os.fsencode(file_path)\n",
    "all_files = Path(file_path).glob('*.csv')\n",
    "\n",
    "# Concatenate it\n",
    "dinosoarin = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Zone'] = 'The Lost World'\n",
    "    dinosoarin = pd.concat([dinosoarin, df], ignore_index=True)\n",
    "\n",
    "\n",
    "## Dino-soarin'\n",
    "file_path = \"/Users/cheryl/Downloads/DSA3101/data/Jurassic Park Rapids Adventure\"\n",
    "directory = os.fsencode(file_path)\n",
    "all_files = Path(file_path).glob('*.csv')\n",
    "\n",
    "# Concatenate it\n",
    "jurassic = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Zone'] = 'The Lost World'\n",
    "    jurassic = pd.concat([jurassic, df], ignore_index=True)\n",
    "\n",
    "\n",
    "## Concatenate The Lost World dataframes\n",
    "lost_world = pd.concat([canopy, dinosoarin, jurassic], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zone: Far Far Away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enchanted Airways\n",
    "file_path = \"/Users/cheryl/Downloads/DSA3101/data/Enchanted Airways\"\n",
    "directory = os.fsencode(file_path)\n",
    "all_files = Path(file_path).glob('*.csv')\n",
    "\n",
    "# Concatenate it\n",
    "enchanted = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Zone'] = 'Far Far Away'\n",
    "    enchanted = pd.concat([enchanted, df], ignore_index=True)\n",
    "\n",
    "\n",
    "## Magic Potion Spin\n",
    "file_path = \"/Users/cheryl/Downloads/DSA3101/data/Magic Potion Spin\"\n",
    "directory = os.fsencode(file_path)\n",
    "all_files = Path(file_path).glob('*.csv')\n",
    "\n",
    "# Concatenate it\n",
    "magic = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Zone'] = 'Far Far Away'\n",
    "    magic = pd.concat([magic, df], ignore_index=True)\n",
    "\n",
    "    \n",
    "## Puss In Boots' Giant Journey\n",
    "file_path = \"/Users/cheryl/Downloads/DSA3101/data/Puss In Bootsâ€™ Giant Journey\"\n",
    "directory = os.fsencode(file_path)\n",
    "all_files = Path(file_path).glob('*.csv')\n",
    "\n",
    "# Concatenate it\n",
    "pussinboots = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Zone'] = 'Far Far Away'\n",
    "    pussinboots = pd.concat([pussinboots, df], ignore_index=True)\n",
    "\n",
    "## Shrek 4-D Adventure\n",
    "file_path = \"/Users/cheryl/Downloads/DSA3101/data/Shrek 4-D Adventure\"\n",
    "directory = os.fsencode(file_path)\n",
    "all_files = Path(file_path).glob('*.csv')\n",
    "\n",
    "# Concatenate it\n",
    "shrek = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Zone'] = 'Far Far Away'\n",
    "    shrek = pd.concat([shrek, df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Concatenate Far Far Away dataframes\n",
    "far_far_away = pd.concat([enchanted, magic, pussinboots, shrek], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zone: New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Lights, Camera, Action!\n",
    "file_path = \"/Users/cheryl/Downloads/DSA3101/data/Lights, Camera, Action!\"\n",
    "directory = os.fsencode(file_path)\n",
    "all_files = Path(file_path).glob('*.csv')\n",
    "\n",
    "# Concatenate it\n",
    "lca = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Zone'] = 'New York'\n",
    "    lca = pd.concat([lca, df], ignore_index=True)\n",
    "\n",
    "\n",
    "## Sesame Street Spaghetti Space Chase\n",
    "file_path = \"/Users/cheryl/Downloads/DSA3101/data/Sesame Street Spaghetti Space Chase\"\n",
    "directory = os.fsencode(file_path)\n",
    "all_files = Path(file_path).glob('*.csv')\n",
    "\n",
    "# Concatenate it\n",
    "sesame = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Zone'] = 'New York'\n",
    "    sesame = pd.concat([sesame, df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Concatenate New York dataframes\n",
    "new_york = pd.concat([lca, sesame], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zone: Ancient Egypt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Treasure Hunters\n",
    "file_path = \"/Users/cheryl/Downloads/DSA3101/data/Treasure Hunters\"\n",
    "directory = os.fsencode(file_path)\n",
    "all_files = Path(file_path).glob('*.csv')\n",
    "\n",
    "# Concatenate it\n",
    "treasure = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Zone'] = 'Ancient Egypt'\n",
    "    treasure = pd.concat([treasure, df], ignore_index=True)\n",
    "\n",
    "\n",
    "## Revenge of the Mummy\n",
    "file_path = \"/Users/cheryl/Downloads/DSA3101/data/Revenge of the Mummy\"\n",
    "directory = os.fsencode(file_path)\n",
    "all_files = Path(file_path).glob('*.csv')\n",
    "\n",
    "# Concatenate it\n",
    "revenge = pd.DataFrame()\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Zone'] = 'Ancient Egypt'\n",
    "    revenge = pd.concat([revenge, df], ignore_index=True)\n",
    "\n",
    "\n",
    "## Concatenate Ancient Egypt dataframes\n",
    "ancient_egypt = pd.concat([treasure, revenge], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Concatanate all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Concatenation\n",
    "df = pd.concat([sci_fi_city, lost_world, far_far_away, new_york, ancient_egypt], ignore_index=True)\n",
    "df.to_csv('waittime_rawdata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Obtain Average Hourly Waittime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date/Time' to datetime\n",
    "df['Date/Time'] = pd.to_datetime(df['Date/Time'])\n",
    "\n",
    "# Split into 'Date' and 'Time' columns\n",
    "df['Date'] = df['Date/Time'].dt.date\n",
    "df['Time'] = df['Date/Time'].dt.time\n",
    "\n",
    "# Date range (2024-01-01 to 2025-03-01)\n",
    "start_date = '2024-01-01'\n",
    "end_date = '2025-03-01'\n",
    "\n",
    "# Opening hours (10 AM - 8 PM)\n",
    "start_time = time(10, 0, 0) \n",
    "end_time = time(20, 0, 0)  \n",
    "\n",
    "# Apply filter\n",
    "df = df[(df['Date/Time'] >= start_date) & (df['Date/Time'] <= end_date)]\n",
    "df = df[(df[\"Time\"] >= start_time) & (df[\"Time\"] <= end_time)]\n",
    "\n",
    "# Group by Date, Hour, Ride, and Zone, then compute average wait time\n",
    "df = df.groupby([\"Date\", df[\"Date/Time\"].dt.hour, \"Ride\", \"Zone\"]).mean({\"Wait Time\": \"mean\"}).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Synthetic Data Generation \n",
    "### Generate visitor count using wait time with Faker\n",
    "- based on survey data Q6: Select the type of attractions you enjoy the most when visiting USS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  Date/Time                                           Ride  \\\n",
      "0      2024-01-01         10                                    Accelerator   \n",
      "1      2024-01-01         10                    Battlestar Galactica: CYLON   \n",
      "2      2024-01-01         10                    Battlestar Galactica: HUMAN   \n",
      "3      2024-01-01         10                                   Canopy Flyer   \n",
      "4      2024-01-01         10                                    Dino-Soarin   \n",
      "...           ...        ...                                            ...   \n",
      "47957  2025-02-28         19                           Revenge of the Mummy   \n",
      "47958  2025-02-28         19            Sesame Street Spaghetti Space Chase   \n",
      "47959  2025-02-28         19                            Shrek 4-D Adventure   \n",
      "47960  2025-02-28         19  TRANSFORMERS The Ride: The Ultimate 3D Battle   \n",
      "47961  2025-02-28         19                               Treasure Hunters   \n",
      "\n",
      "                 Zone  Wait Time  Visitor Count  \n",
      "0         Sci-Fi City   5.000000             67  \n",
      "1         Sci-Fi City   5.000000            144  \n",
      "2         Sci-Fi City   6.666667            145  \n",
      "3      The Lost World   5.000000             53  \n",
      "4      The Lost World   5.000000             19  \n",
      "...               ...        ...            ...  \n",
      "47957   Ancient Egypt   0.000000             79  \n",
      "47958        New York   0.000000             53  \n",
      "47959    Far Far Away   0.000000             96  \n",
      "47960     Sci-Fi City   0.000000            148  \n",
      "47961   Ancient Egypt   0.000000             65  \n",
      "\n",
      "[47962 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Function to generate visitor count based on wait time and popularity of ride\n",
    "def generate_visitor_count(row):\n",
    "\n",
    "    ride_name = row[\"Ride\"]\n",
    "    wait_time = row[\"Wait Time\"]\n",
    "\n",
    "    popular_rides = [\"Battlestar Galactica: CYLON\", \"Battlestar Galactica: HUMAN\", \"TRANSFORMERS The Ride: The Ultimate 3D Battle\", \"Shrek 4-D Adventure\", \"Revenge of the Mummy\", \"Enchanted Airways\"]\n",
    "    medium_rides = [\"Lights, Camera, Action! Hosted by Steven Spielberg\", \"Jurassic Park Rapids Adventure\", \"Puss In Bootsâ€™ Giant Journey\", \"Enchanted Airways\", \"Dino-Soarin\"]\n",
    "    low_rides = [\"Accelerator\", \"Treasure Hunters\", \"Sesame Street Spaghetti Space Chase\"]\n",
    "\n",
    "    if ride_name in popular_rides:\n",
    "        multiplier = 1.5  \n",
    "    elif ride_name in medium_rides:\n",
    "        multiplier = 1.0  \n",
    "    else:\n",
    "        multiplier = 0.7  \n",
    "\n",
    "    if wait_time <= 10:\n",
    "        base_count = fake.random_int(min=10, max=100)\n",
    "    elif wait_time <= 30:\n",
    "        base_count = fake.random_int(min=100, max=500)\n",
    "    elif wait_time <= 60:\n",
    "        base_count = fake.random_int(min=500, max=1500)\n",
    "    else:\n",
    "        base_count = fake.random_int(min=1500, max=3000)\n",
    "\n",
    "    return int(base_count * multiplier) \n",
    "\n",
    "# Apply function to generate visitor counts\n",
    "df[\"Visitor Count\"] = df.apply(generate_visitor_count, axis = 1)\n",
    "print(df)\n",
    "df.to_csv(\"waittime_cleandata.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
